NAME: Samuel Shen
EMAIL: sam.shen321@gmail.com
ID: 405325252

This tarball contains:
    Source file lab2_list.c, which I modified to partition the list into smaller lists
    Makefile with targets:
        test, to run the generate_list.py script to generate results
        profile, to run the profiling tools
        graphs, to create the graphs necessary
        dist, to create the tarball
        clean, to remove the tarball and executables
    lab2b_list.csv, which contains all the test data
    profile.out, which contains the cpu profile information
    graphs.png, which are the specified graphs generated by the script lab2b_list.gp
    README
    lab2b_list.gp, which I modified from the previous lab to graph the necessary info
    

2.3.1.
    In the 1 and 2 thread list tests, the CPU is most likely spent doing the actual
    list operations, which is shown in the first graph since the throughput is much
    higher. With minimal threads, there is not a lot of time spent locking and
    waiting on locks. The expensive parts of the code are the list operations because
    in order to perform them successfully, you need mutual exclusion between threads, and
    the list operations themselves are costly because you need to look through the entire
    list. The CPU in the high-thread spin lock tests is probably spent
    spinning and waiting to acquire locks because with lots of threads, there is much
    higher contention, which results in lower throughput. For mutexes, the CPU may be
    spending a lot of time yielding and context switching..

2.3.2
    When the spin-lock version of the list exerciser is run with a large number of threads,
    my function getLockWaitTime is consuming the most amount of CPU time. This is the function
    I used to wait to acquire mutex or spin locks. This makes sense because there is a lot of contention,
    so the waiting time for locks will be much longer.

2.3.3.
    The average lock wait time rises very dramatically with an increased number of contending threads
    because you now have many different threads all waiting to use the same resource section. With
    greater participants, there is simply less of the resource to go around, and therefore everyone
    has to wait longer. The average completion time per operation rises with the number of contending
    threads less dramatically because although there is more overhead with waiting and acquiring locks,
    it is still continuously making progress on each chunk of operations. The wait time per operation
    goes up faster than the completion time per operation because with each thread, we are recording
    the total wait time of acquiring locks, which means that there are times that are overlapping
    with each other. In the completion time, there is no overlap since is it just a global running
    time for the main thread, so it goes up less quickly.

2.3.4
    The change in performance of the synchronized methods as a function of the number of lists is due
    to the fact that we are now essentially implementing finer grained locking mechanisms. With more lists,
    we now have an individual mutex or spin lock for each of these lists, and therefore we don't need to fight
    over a single lock anymore. This significantly reduces the contention time between threads and improves
    performance overall. The throughout would continue increasing until it hits a plateau, as evidenced
    by graph 4. With more and more lists, the throughput is increasing less and less. This is because eventually
    the overhead from working with all these lists will be greater than the overhead of contention. Looking at
    the graphs, it does appear to be somewhat true that the throughput of an N way partitioned list should
    be equivalent to the throughput of a single list with 1/N threads.
    